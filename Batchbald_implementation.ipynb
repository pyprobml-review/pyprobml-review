{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Batchbald_implementation.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq torch\n",
    "    import torch\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ],
   "metadata": {
    "id": "IX7MRmURPfnD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlDZX-zwMp2r"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq tqdm\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "try:\n",
    "    from torchvision import datasets, transforms\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq torchvision\n",
    "\n",
    "try:\n",
    "    from batchbald_redux import (\n",
    "        active_learning,\n",
    "        batchbald,\n",
    "        consistent_mc_dropout,\n",
    "        joint_entropy,\n",
    "        repeated_mnist,\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq batchbald_redux\n",
    "    from batchbald_redux import (\n",
    "        active_learning,\n",
    "        batchbald,\n",
    "        consistent_mc_dropout,\n",
    "        joint_entropy,\n",
    "        repeated_mnist,\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "    from probml_utils import savefig, latexify\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "    from probml_utils import savefig, latexify\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "latexify(width_scale_factor=2, fig_height=2)"
   ],
   "metadata": {
    "id": "DWTNkCqBVPeF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BayesianCNN(consistent_mc_dropout.BayesianModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv1_drop = consistent_mc_dropout.ConsistentMCDropout2d()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = consistent_mc_dropout.ConsistentMCDropout2d()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc1_drop = consistent_mc_dropout.ConsistentMCDropout()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def mc_forward_impl(self, input: torch.Tensor):\n",
    "        input = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(input)), 2))\n",
    "        input = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(input)), 2))\n",
    "        input = input.view(-1, 1024)\n",
    "        input = F.relu(self.fc1_drop(self.fc1(input)))\n",
    "        input = self.fc2(input)\n",
    "        input = F.log_softmax(input, dim=1)\n",
    "\n",
    "        return input"
   ],
   "metadata": {
    "id": "2QwOlGEBNIXN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_initial_samples = 20  # Number of initial samples required\n",
    "num_classes = 10  # Total classes in MNIST dataset\n",
    "\n",
    "train_dataset, test_dataset = repeated_mnist.create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)\n",
    "\n",
    "# Generates 20 samples (2 from each class) and returns their indices\n",
    "initial_samples = active_learning.get_balanced_sample_indices(\n",
    "    repeated_mnist.get_targets(train_dataset), num_classes=num_classes, n_per_digit=num_initial_samples / num_classes\n",
    ")"
   ],
   "metadata": {
    "id": "IJObu-nIQUvE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(initial_samples)"
   ],
   "metadata": {
    "id": "UittuOqxgaB5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_training_samples = 180  # Maximum limit of train samples needed\n",
    "acquisition_batch_size = 4  # Batch size per iteration\n",
    "num_inference_samples = 100\n",
    "num_test_inference_samples = 5\n",
    "num_samples = 100000  # Total number of samples\n",
    "\n",
    "test_batch_size = 512  # Test Loader Batch size\n",
    "batch_size = 64  # Train loader Batch size\n",
    "scoring_batch_size = 128  # Pool Loader Batch size\n",
    "training_iterations = 4096 * 6\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(f\"use_cuda: {use_cuda}\")\n",
    "\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "kwargs = {\"num_workers\": 0, \"pin_memory\": True} if use_cuda else {}"
   ],
   "metadata": {
    "id": "uHydUWKPNYfe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "active_learning_data = active_learning.ActiveLearningData(\n",
    "    train_dataset\n",
    ")  # Splits the dataset into training dataset and pool dataset\n",
    "\n",
    "active_learning_data.acquire(\n",
    "    initial_samples\n",
    ")  # Seperates the initial indices from the pool and fixes it as initial train dataset\n",
    "active_learning_data.extract_dataset_from_pool(\n",
    "    40000\n",
    ")  # Extracts 40000 samples from pool and makes it as validation dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.training_dataset,\n",
    "    sampler=active_learning.RandomFixedLengthSampler(active_learning_data.training_dataset, training_iterations),\n",
    "    batch_size=batch_size,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "pool_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.pool_dataset, batch_size=scoring_batch_size, shuffle=False, **kwargs\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **kwargs)"
   ],
   "metadata": {
    "id": "POwMf_zCQqZ-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_accs = []\n",
    "test_loss = []\n",
    "added_indices = []"
   ],
   "metadata": {
    "id": "iNS0EvPERvKf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pbar = tqdm(initial=len(active_learning_data.training_dataset), total=max_training_samples, desc=\"Training Set Size\")\n",
    "\n",
    "while True:\n",
    "    model = BayesianCNN(num_classes).to(device=device)  # initialise model\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Train\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data, 1).squeeze(1)\n",
    "        loss = F.nll_loss(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            data = data.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "\n",
    "            prediction = torch.logsumexp(model(data, num_test_inference_samples), dim=1) - math.log(\n",
    "                num_test_inference_samples\n",
    "            )\n",
    "            loss += F.nll_loss(prediction, target, reduction=\"sum\")\n",
    "\n",
    "            prediction = prediction.max(1)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "    test_accs.append(percentage_correct)\n",
    "\n",
    "    print(\"Test set: Average loss: {:.4f}, Accuracy: ({:.2f}%)\".format(loss, percentage_correct))\n",
    "\n",
    "    if len(active_learning_data.training_dataset) >= max_training_samples:\n",
    "        break\n",
    "\n",
    "    # Acquire pool predictions\n",
    "    N = len(active_learning_data.pool_dataset)\n",
    "    logits_N_K_C = torch.empty((N, num_inference_samples, num_classes), dtype=torch.double, pin_memory=use_cuda)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i, (data, _) in enumerate(tqdm(pool_loader, desc=\"Evaluating Acquisition Set\", leave=False)):\n",
    "            data = data.to(device=device)\n",
    "\n",
    "            lower = i * pool_loader.batch_size\n",
    "            upper = min(lower + pool_loader.batch_size, N)\n",
    "            logits_N_K_C[lower:upper].copy_(model(data, num_inference_samples).double(), non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Uncomment for BatchBALD Settings:\n",
    "        candidate_batch = batchbald.get_batchbald_batch(\n",
    "            logits_N_K_C,\n",
    "            acquisition_batch_size,\n",
    "            num_samples,\n",
    "            dtype=torch.double,\n",
    "            device=device,  # Returns the indices and scores(Mutual Information) for the batch selected by Batchbald/BALD Strategy.\n",
    "        )\n",
    "\n",
    "        # Uncomment for BALD Settings:\n",
    "        # candidate_batch = batchbald.get_bald_batch(\n",
    "        #     logits_N_K_C, acquisition_batch_size, dtype=torch.double, device=device\n",
    "        # )\n",
    "\n",
    "    targets = repeated_mnist.get_targets(active_learning_data.pool_dataset)  # Returns the target labels\n",
    "    dataset_indices = active_learning_data.get_dataset_indices(\n",
    "        candidate_batch.indices\n",
    "    )  # Returns indices for candidate batch\n",
    "\n",
    "    print(\"Dataset indices: \", dataset_indices)\n",
    "    # print(\"Scores: \", candidate_batch.scores)\n",
    "    print(\"Labels: \", targets[candidate_batch.indices])\n",
    "\n",
    "    active_learning_data.acquire(candidate_batch.indices)  # add the new indices to training dataset\n",
    "    added_indices.append(dataset_indices)\n",
    "    pbar.update(len(dataset_indices))"
   ],
   "metadata": {
    "id": "3GlhSH8uUleI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(test_accs)"
   ],
   "metadata": {
    "id": "JCvsNW6dejAc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "p = plt.rcParams\n",
    "p[\"axes.grid\"] = True\n",
    "p[\"grid.color\"] = \"#999999\"\n",
    "p[\"grid.linestyle\"] = \"--\"\n",
    "\n",
    "p[\"lines.linewidth\"] = 2\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, 164, 4), test_accs, label=\"16\")\n",
    "\n",
    "plt.legend(title=\"Batch-Size\", loc=\"lower right\")\n",
    "plt.xlabel(\"No. of Points Queried\", fontsize=9)\n",
    "plt.ylabel(\"Test Accuracy\", fontsize=9)\n",
    "plt.xticks([i for i in range(0, 164, 14)], rotation=90)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "savefig(\"test_accuracy_curve\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "492b7LXsaY0P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rows = [\"Batch {}\".format(row) for row in [1, 2, 3]]\n",
    "plt.rcParams[\"axes.titlesize\"] = 10\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4)\n",
    "plot_indices = [added_indices[i][j] for i in range(0, 3) for j in range(0, 4)]\n",
    "\n",
    "for i, ax in zip(range(1, 4 * 3 + 1), axes.flatten()):\n",
    "    image = train_dataset[plot_indices[i - 1]][0].reshape((28, 28))\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.grid(False)\n",
    "    ax.tick_params(axis=\"both\", labelsize=0, length=0, left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "for ax, row in zip(axes[:, 0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size=\"large\")\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "savefig(\"batchbald_samples\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "XT28W1YmdtcB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "XQt0wEW2TGbx"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}